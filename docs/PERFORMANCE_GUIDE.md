# Performance Optimization Guide

This guide provides comprehensive optimization strategies for the anthropic-proxy's intelligent context management system to achieve maximum performance and efficiency.

## Performance Overview

### üöÄ Current Performance Benchmarks

| Metric | Current Performance | Target | Notes |
|--------|-------------------|--------|-------|
| **Token Processing Speed** | 9,079 tokens/sec | 10,000+ tokens/sec | With intelligent context management |
| **Token Counting Accuracy** | 95%+ | 95%+ | tiktoken-based precise calculation |
| **Cache Hit Ratio** | 98.7% | 95%+ | For AI condensation operations |
| **Response Time** | <50ms additional latency | <100ms | For context management |
| **Token Savings** | Up to 122 tokens per condensation | 100+ tokens | AI-powered condensation |
| **Memory Efficiency** | Configurable LRU caches | Optimized per workload | Automatic cleanup |

### üéØ Performance Components

1. **Token Counting Engine**: tiktoken-based accurate counting with LRU caching
2. **AI Condensation System**: Async AI-powered message summarization
3. **Context Analysis Engine**: Multi-level risk assessment
4. **Caching Layer**: Multi-tier caching with file-based persistence
5. **Async Processing**: Non-blocking operations for minimal latency

## Configuration Optimization

### üß† Intelligent Context Management

#### High-Traffic Production
```bash
# Optimized for high throughput
ENABLE_AI_CONDENSATION=true
CONDENSATION_DEFAULT_STRATEGY=conversation_summary
CONDENSATION_CAUTION_THRESHOLD=0.70
CONDENSATION_WARNING_THRESHOLD=0.80
CONDENSATION_CRITICAL_THRESHOLD=0.90
CONDENSATION_MIN_MESSAGES=3
CONDENSATION_MAX_MESSAGES=5              # Reduced for faster processing
CONDENSATION_TIMEOUT=15                   # Reduced timeout
CONDENSATION_CACHE_TTL=1800               # Shorter TTL for fresher content
```

#### Maximum Accuracy
```bash
# Optimized for accuracy over speed
ENABLE_AI_CONDENSATION=true
CONDENSATION_DEFAULT_STRATEGY=key_point_extraction
CONDENSATION_QUALITY_THRESHOLD=0.9        # Higher quality threshold
CONDENSATION_MIN_MESSAGES=5               # More messages for better context
CONDENSATION_MAX_MESSAGES=15              # Process more messages
CONDENSATION_TIMEOUT=60                   # Longer timeout for quality
```

#### Development/Testing
```bash
# Optimized for debugging and monitoring
ENABLE_AI_CONDENSATION=true
ENABLE_CONTEXT_PERFORMANCE_LOGGING=true
ENABLE_TOKEN_COUNTING_LOGGING=true
CONDENSATION_CAUTION_THRESHOLD=0.30        # Lower thresholds for testing
CONDENSATION_WARNING_THRESHOLD=0.50
CONDENSATION_CRITICAL_THRESHOLD=0.70
```

### üéØ Token Counting Optimization

#### High-Performance Settings
```bash
# Optimized for speed
ENABLE_ACCURATE_TOKEN_COUNTING=true
TIKTOKEN_CACHE_SIZE=2000                   # Larger cache for better hit ratio
ENABLE_TOKEN_COUNTING_LOGGING=false        # Disable logging for performance
BASE_IMAGE_TOKENS=85
IMAGE_TOKENS_PER_CHAR=0.25
ENABLE_DYNAMIC_IMAGE_TOKENS=true
```

#### Memory-Constrained Settings
```bash
# Optimized for low memory usage
ENABLE_ACCURATE_TOKEN_COUNTING=true
TIKTOKEN_CACHE_SIZE=500                    # Smaller cache
ENABLE_DYNAMIC_IMAGE_TOKENS=false          # Use fixed calculation
BASE_IMAGE_TOKENS=1000                     # Fixed calculation
```

### üìä Caching Optimization

#### High-Performance Caching
```bash
# Optimized for maximum cache performance
CONTEXT_CACHE_SIZE=200                     # Larger context cache
CONTEXT_ANALYSIS_CACHE_TTL=600             # Longer TTL
CONDENSATION_CACHE_TTL=7200               # 2 hours for condensation results
CACHE_ENABLE_LOGGING=false                 # Disable cache logging
```

#### Memory-Efficient Caching
```bash
# Optimized for memory efficiency
CONTEXT_CACHE_SIZE=50
CONTEXT_ANALYSIS_CACHE_TTL=180
CONDENSATION_CACHE_TTL=1800
```

## Performance Monitoring

### üìà Key Performance Indicators

Monitor these metrics to ensure optimal performance:

#### Core Metrics
```python
def get_performance_metrics():
    """Get comprehensive performance metrics"""
    
    from context_window_manager import get_context_performance_stats
    
    stats = get_context_performance_stats()
    
    return {
        'cache_hit_ratio': stats.get('cache_hit_ratio', 0),
        'condensation_success_rate': stats.get('condensation_stats', {}).get('success_rate', 0),
        'avg_processing_time_ms': stats.get('condensation_stats', {}).get('avg_processing_time_ms', 0),
        'tokens_saved_per_request': stats.get('condensation_stats', {}).get('avg_tokens_saved', 0),
        'memory_usage_mb': stats.get('memory_usage_mb', 0),
        'error_rate': stats.get('error_rate', 0)
    }

# Example monitoring
metrics = get_performance_metrics()
print(f"Cache Hit Ratio: {metrics['cache_hit_ratio']:.1%}")
print(f"Condensation Success Rate: {metrics['condensation_success_rate']:.1%}")
print(f"Avg Processing Time: {metrics['avg_processing_time_ms']:.1f}ms")
```

#### Target Performance Levels
- **Cache Hit Ratio**: >95% (excellent), >80% (good), <80% (needs optimization)
- **Condensation Success Rate**: >95% (excellent), >90% (good), <90% (needs attention)
- **Processing Time**: <50ms (excellent), <100ms (good), >100ms (needs optimization)
- **Error Rate**: <1% (excellent), <5% (good), >5% (needs attention)

### üîç Real-time Monitoring

#### Performance Monitoring Script
```python
import time
import asyncio
from context_window_manager import apply_intelligent_context_management, get_context_performance_stats

class PerformanceMonitor:
    def __init__(self):
        self.metrics = {
            'request_count': 0,
            'total_processing_time': 0,
            'total_tokens_saved': 0,
            'errors': 0,
            'start_time': time.time()
        }
    
    async def monitored_request(self, messages, is_vision=False):
        """Monitor a single request"""
        
        start_time = time.time()
        
        try:
            result = await apply_intelligent_context_management(
                messages, is_vision=is_vision
            )
            
            processing_time = (time.time() - start_time) * 1000
            
            # Update metrics
            self.metrics['request_count'] += 1
            self.metrics['total_processing_time'] += processing_time
            self.metrics['total_tokens_saved'] += result.tokens_saved
            
            return result
            
        except Exception as e:
            self.metrics['errors'] += 1
            raise e
    
    def get_performance_summary(self):
        """Get performance summary"""
        
        runtime = time.time() - self.metrics['start_time']
        
        if self.metrics['request_count'] == 0:
            return {"status": "No requests processed"}
        
        return {
            'requests_per_second': self.metrics['request_count'] / runtime,
            'avg_processing_time_ms': self.metrics['total_processing_time'] / self.metrics['request_count'],
            'avg_tokens_saved': self.metrics['total_tokens_saved'] / self.metrics['request_count'],
            'error_rate': self.metrics['errors'] / self.metrics['request_count'],
            'total_requests': self.metrics['request_count'],
            'runtime_seconds': runtime
        }

# Usage example
monitor = PerformanceMonitor()

async def test_performance():
    # Test with sample data
    test_messages = [
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "Hello! " * 1000},  # Large message
        {"role": "assistant", "content": "I can help you with many things. " * 1000},
    ]
    
    # Run multiple requests
    for i in range(100):
        await monitor.monitored_request(test_messages)
        
        if i % 20 == 0:
            summary = monitor.get_performance_summary()
            print(f"Batch {i//20 + 1}: {summary}")

# Run performance test
asyncio.run(test_performance())
```

## Optimization Strategies

### ‚ö° Token Counting Optimization

#### 1. Cache Optimization
```python
# Optimize token counting cache size based on workload
def optimize_cache_size(requests_per_minute, avg_message_length):
    """Calculate optimal cache size"""
    
    # Base cache size calculation
    base_size = min(2000, max(100, requests_per_minute // 10))
    
    # Adjust for message complexity
    complexity_factor = min(2.0, avg_message_length / 1000)
    
    optimal_size = int(base_size * complexity_factor)
    
    return optimal_size

# Example: For 1000 requests/minute with 2000 char messages
cache_size = optimize_cache_size(1000, 2000)
print(f"Recommended cache size: {cache_size}")
```

#### 2. Batch Processing
```python
import asyncio
from accurate_token_counter import AccurateTokenCounter

async def batch_token_count(messages_batch):
    """Efficient batch token counting"""
    
    counter = AccurateTokenCounter()
    
    # Process multiple messages concurrently
    tasks = [
        counter.count_tokens_async(message) 
        for message in messages_batch
    ]
    
    results = await asyncio.gather(*tasks)
    
    return results

# Usage example
messages = [{"content": "Hello " * 100} for _ in range(10)]
token_counts = await batch_token_count(messages)
```

### üß† AI Condensation Optimization

#### 1. Strategy Selection
```python
def select_optimal_strategy(context_utilization, message_count, content_type):
    """Select optimal condensation strategy based on context"""
    
    if context_utilization < 0.80:
        return None  # No condensation needed
    
    if content_type == "code" or content_type == "technical":
        return "key_point_extraction"  # Preserve technical details
    
    if message_count > 20:
        return "progressive_summarization"  # Gradual summarization
    
    if context_utilization > 0.95:
        return "smart_truncation"  # Fastest option
    
    return "conversation_summary"  # Default balanced approach

# Usage example
strategy = select_optimal_strategy(0.85, 15, "general")
print(f"Recommended strategy: {strategy}")
```

#### 2. Quality vs Speed Trade-off
```bash
# High quality (slower)
CONDENSATION_TIMEOUT=60
CONDENSATION_MAX_MESSAGES=15
CONDENSATION_QUALITY_THRESHOLD=0.9

# Balanced (default)
CONDENSATION_TIMEOUT=30
CONDENSATION_MAX_MESSAGES=10
CONDENSATION_QUALITY_THRESHOLD=0.8

# High speed (lower quality)
CONDENSATION_TIMEOUT=15
CONDENSATION_MAX_MESSAGES=5
CONDENSATION_QUALITY_THRESHOLD=0.6
```

### üóÑÔ∏è Caching Optimization

#### 1. Multi-tier Caching Strategy
```python
class MultiTierCache:
    def __init__(self):
        self.memory_cache = {}      # L1: In-memory cache
        self.file_cache = {}        # L2: File-based cache
        self.redis_cache = {}       # L3: Redis cache (optional)
    
    async def get(self, key):
        # Try L1 cache first
        if key in self.memory_cache:
            return self.memory_cache[key]
        
        # Try L2 cache
        if key in self.file_cache:
            value = await self.file_cache[key]
            self.memory_cache[key] = value  # Promote to L1
            return value
        
        # Try L3 cache (if available)
        if self.redis_cache and key in self.redis_cache:
            value = await self.redis_cache[key]
            self.memory_cache[key] = value  # Promote to L1
            self.file_cache[key] = value    # Store in L2
            return value
        
        return None
    
    async def set(self, key, value, ttl=3600):
        # Store in all tiers
        self.memory_cache[key] = value
        self.file_cache[key] = value
        
        if self.redis_cache:
            await self.redis_cache.set(key, value, ttl)
```

#### 2. Cache Warming Strategies
```python
async def warm_up_caches():
    """Warm up caches with common patterns"""
    
    from context_window_manager import get_context_performance_stats
    
    # Pre-populate token counting cache
    common_messages = [
        "Hello, how are you?",
        "Can you help me with...",
        "Thank you for your help.",
        # Add more common patterns
    ]
    
    counter = AccurateTokenCounter()
    for message in common_messages:
        await counter.count_tokens_async({"content": message})
    
    print("Cache warming completed")

# Warm up caches on startup
asyncio.run(warm_up_caches())
```

## Performance Testing

### üß™ Load Testing

#### Basic Load Test
```python
import asyncio
import time
import aiohttp

async def load_test(base_url, concurrent_requests=50, total_requests=1000):
    """Perform load testing"""
    
    async def single_request(session, request_id):
        start_time = time.time()
        
        try:
            async with session.post(
                f"{base_url}/v1/chat/completions",
                json={
                    "model": "glm-4.5",
                    "messages": [
                        {"role": "user", "content": f"Test message {request_id}"}
                    ],
                    "max_tokens": 100
                },
                headers={"Authorization": "Bearer test_key"}
            ) as response:
                data = await response.json()
                processing_time = (time.time() - start_time) * 1000
                
                return {
                    "request_id": request_id,
                    "status": response.status,
                    "processing_time_ms": processing_time,
                    "tokens_used": data.get("usage", {}).get("total_tokens", 0)
                }
        except Exception as e:
            return {
                "request_id": request_id,
                "status": "error",
                "error": str(e),
                "processing_time_ms": (time.time() - start_time) * 1000
            }
    
    # Run concurrent requests
    connector = aiohttp.TCPConnector(limit=concurrent_requests)
    async with aiohttp.ClientSession(connector=connector) as session:
        
        tasks = []
        for i in range(total_requests):
            task = single_request(session, i)
            tasks.append(task)
        
        results = await asyncio.gather(*tasks)
        
        # Analyze results
        successful_requests = [r for r in results if r["status"] == 200]
        failed_requests = [r for r in results if r["status"] != 200]
        
        if successful_requests:
            avg_processing_time = sum(r["processing_time_ms"] for r in successful_requests) / len(successful_requests)
            max_processing_time = max(r["processing_time_ms"] for r in successful_requests)
            min_processing_time = min(r["processing_time_ms"] for r in successful_requests)
        else:
            avg_processing_time = max_processing_time = min_processing_time = 0
        
        return {
            "total_requests": total_requests,
            "successful_requests": len(successful_requests),
            "failed_requests": len(failed_requests),
            "success_rate": len(successful_requests) / total_requests,
            "avg_processing_time_ms": avg_processing_time,
            "max_processing_time_ms": max_processing_time,
            "min_processing_time_ms": min_processing_time,
            "requests_per_second": total_requests / (time.time() - start_time)
        }

# Run load test
results = asyncio.run(load_test("http://localhost:5000"))
print(f"Load test results: {results}")
```

### üìä Performance Benchmarks

#### Comprehensive Benchmark Suite
```python
class PerformanceBenchmark:
    def __init__(self):
        self.results = {}
    
    async def benchmark_token_counting(self):
        """Benchmark token counting performance"""
        
        from accurate_token_counter import AccurateTokenCounter
        
        counter = AccurateTokenCounter()
        test_messages = [
            {"content": "Short message"},
            {"content": "Medium length message " * 50},
            {"content": "Very long message " * 500},
            {"content": [{"type": "text", "text": "Message with image"}]},
        ]
        
        results = {}
        
        for i, message in enumerate(test_messages):
            start_time = time.time()
            
            for _ in range(100):  # Run 100 iterations
                await counter.count_tokens_async(message)
            
            end_time = time.time()
            
            results[f"message_type_{i}"] = {
                "avg_time_ms": (end_time - start_time) * 1000 / 100,
                "tokens_per_second": 100 / (end_time - start_time)
            }
        
        self.results["token_counting"] = results
    
    async def benchmark_condensation(self):
        """Benchmark AI condensation performance"""
        
        from context_window_manager import apply_intelligent_context_management
        
        test_conversations = [
            # Small conversation
            [{"role": "user", "content": "Hello"}, {"role": "assistant", "content": "Hi there!"}],
            
            # Medium conversation
            [{"role": "user", "content": "Question " * 100}] * 20,
            
            # Large conversation
            [{"role": "user", "content": "Long message " * 1000}] * 50,
        ]
        
        results = {}
        
        for i, conversation in enumerate(test_conversations):
            start_time = time.time()
            
            result = await apply_intelligent_context_management(
                conversation, is_vision=False
            )
            
            end_time = time.time()
            
            results[f"conversation_size_{i}"] = {
                "processing_time_ms": (end_time - start_time) * 1000,
                "tokens_saved": result.tokens_saved,
                "tokens_per_second": len(conversation) / (end_time - start_time)
            }
        
        self.results["condensation"] = results
    
    async def run_all_benchmarks(self):
        """Run all performance benchmarks"""
        
        print("üöÄ Starting performance benchmarks...")
        
        await self.benchmark_token_counting()
        await self.benchmark_condensation()
        
        print("‚úÖ Benchmarks completed!")
        return self.results

# Run benchmarks
benchmark = PerformanceBenchmark()
results = await benchmark.run_all_benchmarks()
print(f"Benchmark results: {results}")
```

## Troubleshooting Performance Issues

### üîß Common Performance Problems

#### 1. High Memory Usage
**Symptoms**: Memory usage continuously increases
**Causes**: Cache sizes too large, memory leaks
**Solutions**:
```bash
# Reduce cache sizes
CONTEXT_CACHE_SIZE=50
TIKTOKEN_CACHE_SIZE=500
CONDENSATION_CACHE_TTL=1800

# Enable cache cleanup
CACHE_CLEANUP_INTERVAL=1800
```

#### 2. Slow Response Times
**Symptoms**: Requests taking >500ms
**Causes**: AI condensation timeouts, slow cache operations
**Solutions**:
```bash
# Optimize timeouts
CONDENSATION_TIMEOUT=15
CONDENSATION_MAX_MESSAGES=5

# Enable performance monitoring
ENABLE_CONTEXT_PERFORMANCE_LOGGING=true

# Optimize caching
CONTEXT_CACHE_SIZE=200
TIKTOKEN_CACHE_SIZE=2000
```

#### 3. Low Cache Hit Ratios
**Symptoms**: Cache hit ratio <80%
**Causes**: Cache too small, TTL too short, high request variety
**Solutions**:
```bash
# Increase cache sizes
CONTEXT_CACHE_SIZE=500
TIKTOKEN_CACHE_SIZE=3000

# Increase TTL values
CONTEXT_ANALYSIS_CACHE_TTL=1800
CONDENSATION_CACHE_TTL=7200
```

### üìà Performance Optimization Checklist

- [ ] **Cache Configuration**: Optimize cache sizes for workload
- [ ] **Timeout Settings**: Balance between reliability and speed
- [ ] **Monitoring Setup**: Enable performance logging
- [ ] **Load Testing**: Validate performance under expected load
- [ ] **Memory Usage**: Monitor and optimize memory consumption
- [ ] **Error Handling**: Ensure graceful degradation
- [ ] **Benchmarking**: Establish performance baselines
- [ ] **Regular Monitoring**: Set up automated performance alerts

## Conclusion

The intelligent context management system provides excellent performance out of the box, but proper optimization can further enhance its capabilities. Key takeaways:

1. **Monitor Performance**: Regularly check cache hit ratios and processing times
2. **Optimize Configuration**: Tune settings based on your specific workload
3. **Test Thoroughly**: Use load testing to validate performance under stress
4. **Monitor Resources**: Keep an eye on memory usage and error rates
5. **Iterate and Improve**: Continuously optimize based on performance data

With proper optimization and monitoring, the system can handle high-traffic workloads while maintaining excellent accuracy and user experience.