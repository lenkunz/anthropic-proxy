# Anthropic Proxy Environment Configuration (example)
# Copy this file to .env and configure your settings

# Upstream endpoints
UPSTREAM_BASE=https://api.z.ai/api/anthropic
OPENAI_UPSTREAM_BASE=https://api.z.ai/api/coding/paas/v4

# z.ai Enhanced Features
# Automatically add thinking parameter {"type": "enabled"} to OpenAI requests
# This enables enhanced reasoning capabilities from z.ai models
ENABLE_ZAI_THINKING=true

# Auth (optional) â€” Not required if clients send their own keys.
# SERVER_API_KEY=your-zai-api-key-here
# FORWARD_CLIENT_KEY=true

# Token counting (optional)
# FORWARD_COUNT_TO_UPSTREAM=true
# COUNT_SHAPE_COMPAT=true

# Models (optional)
# AUTOTEXT_MODEL=glm-4.5
# AUTOVISION_MODEL=glm-4.5v
# OPENAI_MODELS_LIST_JSON=["glm-4.5","glm-4.5-openai","glm-4.5-anthropic"]
# MODEL_MAP_JSON={}

# Token scaling configuration (optional)
# Expected token limits for client compatibility
# ANTHROPIC_EXPECTED_TOKENS=200000
# OPENAI_EXPECTED_TOKENS=128000
# Real model context windows for proper scaling
# REAL_TEXT_MODEL_TOKENS=128000
# REAL_VISION_MODEL_TOKENS=65536

# Endpoint preference for text-only requests (optional)
# TEXT_ENDPOINT_PREFERENCE=auto

# Anthropic Beta features (optional)
# FORCE_ANTHROPIC_BETA=false
# DEFAULT_ANTHROPIC_BETA=prompt-caching-2024-07-31