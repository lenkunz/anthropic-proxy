# Anthropic Proxy Environment Configuration
# Copy this file to .env and update with your actual values

# === Core Configuration ===
# Base URL for the upstream Anthropic API
UPSTREAM_BASE=https://api.z.ai/api/anthropic

# Base URL for the upstream OpenAI-compatible API (for image models)
OPENAI_UPSTREAM_BASE=https://api.z.ai/api/coding/paas/v4

# API key for server authentication (required)
# Replace with your actual z.ai API key
SERVER_API_KEY=your_api_key_here

# === Request Forwarding ===
# Whether to forward the client's API key to the upstream service
# Set to true to forward client keys, false to use SERVER_API_KEY only
FORWARD_CLIENT_KEY=true

# Whether to forward count tokens requests to upstream service
# Set to true for accurate token counts, false for local estimation
FORWARD_COUNT_TO_UPSTREAM=true

# === Anthropic Beta Features ===
# Force Anthropic beta headers for all requests
# Set to true for debugging, false for production
DEBUG=false

# Default Anthropic beta header value when beta features are enabled
# Leave as default unless you need specific beta features
DEFAULT_ANTHROPIC_BETA=prompt-caching-2024-07-31

# === Model Configuration ===
# JSON mapping for model aliases (leave empty for no mapping)
# Example: {"custom-name": "actual-model-name"}
MODEL_MAP_JSON={}

# Default models for auto-selection
# AUTOTEXT_MODEL: Used for text-only requests when no model specified
AUTOTEXT_MODEL=glm-4.6

# AUTOVISION_MODEL: Used for requests with images when no model specified
AUTOVISION_MODEL=glm-4.5v

# Text endpoint preference: auto, openai, anthropic
# auto = route based on content type and model suffix (recommended)
# openai = always route text requests to OpenAI endpoint  
# anthropic = always route text requests to Anthropic endpoint
TEXT_ENDPOINT_PREFERENCE=auto

# Enable thinking parameter for z.ai's OpenAI endpoint
# Set to true to add thinking: {"type": "enabled"} to OpenAI requests
# This enables enhanced reasoning capabilities from z.ai models
ENABLE_ZAI_THINKING=true

# === Token Counting ===
# Enable compatibility mode for token counting shape
# Set to true for OpenAI-compatible response format
COUNT_SHAPE_COMPAT=true

# Minimum number of tokens required for cacheable prompts
# Prompts below this threshold won't use caching features
MIN_CACHEABLE_TOKENS=1024

# Scale token counts for vision requests
# Set to true to properly scale tokens between text and vision models
SCALE_COUNT_TOKENS_FOR_VISION=true

# === Token Scaling Configuration ===
# Expected token limits for different endpoints (used for scaling calculations)
# These should match the advertised limits from upstream APIs

# Anthropic endpoint expected token limit
ANTHROPIC_EXPECTED_TOKENS=200000

# OpenAI endpoint expected token limit  
OPENAI_EXPECTED_TOKENS=128000

# Real model context windows (configurable for accuracy)
# CRITICAL: These determine the actual context validation and truncation behavior

# Text model actual context window (Anthropic endpoint)
# Set to 128000 for proper text request handling
REAL_TEXT_MODEL_TOKENS=128000

# Vision model actual context window (OpenAI endpoint)  
# Set to 65536 for proper image request handling
REAL_VISION_MODEL_TOKENS=65536

# === Token Validation Configuration ===
# Token count validation thresholds (applied to text models only, images auto-trim)
# Validates tiktoken estimates against actual API usage for accuracy

# Percentage threshold for token validation errors (default: 10.0%)
# Only trigger error if difference exceeds this percentage AND minimum difference
TOKEN_VALIDATION_PERCENTAGE_THRESHOLD=10.0

# Minimum token difference for validation errors (default: 25 tokens)  
# Only trigger error if difference exceeds this amount AND percentage threshold
# This prevents errors for small requests where percentage differences are misleading
TOKEN_VALIDATION_MIN_DIFFERENCE=25

# === Image Age Management ===
# Number of messages before images are considered "old" and removed
# Higher values keep images longer, lower values remove them sooner
IMAGE_AGE_THRESHOLD=3

# Number of previous messages to include in cache key context
# Used for generating contextual image descriptions
CACHE_CONTEXT_MESSAGES=2

# Template for truncation message when old images are removed
# {descriptions} will be replaced with AI-generated descriptions
IMAGE_AGE_TRUNCATION_MESSAGE="[Previous images in conversation context: {descriptions}]"

# === Cache Configuration ===
# Default cache TTL in seconds (5 minutes)
CACHE_DEFAULT_TTL_SECONDS=300

# Extended cache TTL in seconds (1 hour)
CACHE_1H_TTL_SECONDS=3600

# Maximum number of image descriptions to keep in memory cache
IMAGE_DESCRIPTION_CACHE_SIZE=1000

# Directory for persistent file-based cache
# Should be mounted as Docker volume for persistence
CACHE_DIR=./cache

# Enable detailed cache performance logging
# Set to true for debugging cache performance, false to reduce logs
CACHE_ENABLE_LOGGING=false

# === Configuration Validation Notes ===
# 
# Critical settings for proper operation:
# 1. SERVER_API_KEY must be a valid z.ai API key
# 2. REAL_TEXT_MODEL_TOKENS=128000 for proper text handling
# 3. REAL_VISION_MODEL_TOKENS=65536 for proper image handling  
# 4. TEXT_ENDPOINT_PREFERENCE=auto provides best routing behavior
# 5. FORWARD_CLIENT_KEY=true enables OpenAI client compatibility
#
# Model variants available:
# - glm-4.6: Auto-routing (text → Anthropic, images → OpenAI)
# - glm-4.6-openai: Force OpenAI endpoint for all requests
# - glm-4.6-anthropic: Force Anthropic for text, OpenAI for images
#
# For troubleshooting:
# - Set DEBUG=true for verbose logging
# - Set CACHE_ENABLE_LOGGING=true for cache debugging
# - Check docker-compose logs for routing decisions
#
# Recent fixes:
# - Fixed token limit bug where text requests routed to OpenAI 
#   were using vision model limits (65536) instead of text limits (128K)