#!/usr/bin/env python3
"""
Before/After Comparison - Anthropic Proxy v1.6.0 Fixes

This script demonstrates what was broken before v1.6.0 and what's now working.
"""

import json
import requests
from dotenv import load_dotenv
import os

load_dotenv()
API_KEY = os.getenv("SERVER_API_KEY")
BASE_URL = "http://localhost:5000"
HEADERS = {"Authorization": f"Bearer {API_KEY}", "Content-Type": "application/json"}

def show_before_after():
    print("üîß Anthropic Proxy v1.6.0 - Before/After Comparison")
    print("=" * 55)
    print()
    
    print("‚ùå BEFORE v1.6.0 (What was broken):")
    print("   ‚Ä¢ Complex Anthropic messages with tool calls ‚Üí 500 errors")
    print("   ‚Ä¢ System messages + multipart content ‚Üí conversion failure") 
    print("   ‚Ä¢ Streaming requests ‚Üí 'stream has been closed' errors")
    print("   ‚Ä¢ Missing headers for Anthropic streaming ‚Üí undefined variable errors")
    print("   ‚Ä¢ Tool results and tool calls ‚Üí improper format conversion")
    print()
    
    print("‚úÖ AFTER v1.6.0 (What's now working):")
    print("   ‚Ä¢ Complex message structures ‚Üí proper OpenAI format conversion")
    print("   ‚Ä¢ System messages ‚Üí correctly moved to messages array")
    print("   ‚Ä¢ Tool calls/results ‚Üí proper OpenAI tool format")
    print("   ‚Ä¢ Streaming requests ‚Üí graceful completion with 200 OK")
    print("   ‚Ä¢ All endpoint routing ‚Üí proper headers for both Anthropic/OpenAI")
    print()

def example_previously_failing_request():
    """
    This type of request would have failed before v1.6.0
    """
    print("üß™ Testing Previously Failing Request Type")
    print("-" * 45)
    
    # This exact structure was causing the original error
    problematic_payload = {
        "model": "glm-4.5-openai",  # Force OpenAI routing to test conversion
        "max_tokens": 200,
        "stream": False,
        "system": "You are a helpful assistant with access to development tools.",
        "messages": [
            {
                "role": "user",
                "content": [
                    {"type": "text", "text": "Help me debug this API issue"}
                ]
            },
            {
                "role": "assistant",
                "content": [
                    {"type": "text", "text": "I'll help you debug. Let me check the logs."},
                    {
                        "type": "tool_use",
                        "id": "call_123",
                        "name": "check_logs",
                        "input": {"type": "error_logs"}
                    }
                ]
            },
            {
                "role": "user", 
                "content": [
                    {
                        "type": "tool_result",
                        "tool_use_id": "call_123",
                        "content": "Found stream closure errors"
                    }
                ]
            }
        ]
    }
    
    print("üì§ Sending request that would have failed before v1.6.0...")
    print("   ‚Ä¢ System message: ‚úì")
    print("   ‚Ä¢ Tool calls: ‚úì") 
    print("   ‚Ä¢ Tool results: ‚úì")
    print("   ‚Ä¢ Multipart content: ‚úì")
    print("   ‚Ä¢ OpenAI routing: ‚úì")
    
    try:
        response = requests.post(
            f"{BASE_URL}/v1/messages",
            headers=HEADERS,
            json=problematic_payload,
            timeout=20
        )
        
        print(f"üì• Response: {response.status_code}")
        
        if response.status_code == 200:
            result = response.json()
            print("‚úÖ SUCCESS! This request now works perfectly")
            print(f"   ‚Ä¢ Conversion completed without errors")
            print(f"   ‚Ä¢ Response ID: {result.get('id', 'N/A')}")
            print(f"   ‚Ä¢ Content received: {len(result.get('content', []))} blocks")
            print(f"   ‚Ä¢ Token usage: {result.get('usage', {})}")
        else:
            print(f"‚ùå Still failing: {response.status_code}")
            print(f"   Error: {response.text}")
            
    except Exception as e:
        print(f"‚ùå Request error: {e}")

def example_streaming_fix():
    """
    Show that streaming no longer returns 500 errors
    """
    print("\nüåä Streaming Fix Demonstration")
    print("-" * 32)
    
    streaming_payload = {
        "model": "glm-4.5",  # Auto-routing
        "max_tokens": 100,
        "stream": True,
        "messages": [
            {
                "role": "user",
                "content": "Tell me about the v1.6.0 streaming fixes."
            }
        ]
    }
    
    print("üì§ Sending streaming request...")
    print("   ‚Ä¢ Stream: enabled")
    print("   ‚Ä¢ Model: auto-routing")
    
    try:
        response = requests.post(
            f"{BASE_URL}/v1/messages",
            headers=HEADERS,
            json=streaming_payload,
            stream=True,
            timeout=15
        )
        
        print(f"üì• Response: {response.status_code}")
        
        if response.status_code == 200:
            print("‚úÖ SUCCESS! Streaming now works without errors")
            print("   ‚Ä¢ No 'stream has been closed' 500 errors")
            print("   ‚Ä¢ Graceful stream completion")
            print("   ‚Ä¢ Proper headers for both endpoints")
        else:
            print(f"‚ùå Streaming issue: {response.status_code}")
            print(f"   Response: {response.text}")
            
    except Exception as e:
        print(f"‚ùå Streaming error: {e}")

def main():
    # Check service
    try:
        health = requests.get(f"{BASE_URL}/health", timeout=5)
        if health.status_code != 200:
            print("‚ùå Proxy not running. Start with: docker compose up -d")
            return
    except:
        print("‚ùå Cannot connect to proxy. Start with: docker compose up -d")
        return
    
    show_before_after()
    example_previously_failing_request()
    example_streaming_fix()
    
    print("\n" + "=" * 55)
    print("üìã Summary of v1.6.0 Fixes:")
    print("   ‚úÖ Complex message conversion working")
    print("   ‚úÖ Streaming errors resolved") 
    print("   ‚úÖ All endpoint routing functional")
    print("   ‚úÖ Backward compatibility maintained")
    print()
    print("üéâ The anthropic-proxy is now more robust and handles")
    print("   complex message structures that were previously failing!")

if __name__ == "__main__":
    main()