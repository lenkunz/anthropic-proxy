#!/usr/bin/env python3
"""
Example demonstrating image model routing and token scaling.
Shows how the proxy automatically routes different request types to appropriate endpoints.
"""

import asyncio
import httpx
import json
import base64
import os

# Configuration
PROXY_BASE_URL = "http://localhost:5000"
API_KEY = os.getenv("API_KEY", "your-api-key-here")

# Sample base64 encoded 1x1 pixel image (PNG)
SAMPLE_IMAGE_B64 = "iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mP8/5+hHgAHggJ/PchI7wAAAABJRU5ErkJggg=="

def get_headers():
    return {
        "Authorization": f"Bearer {API_KEY}",
        "Content-Type": "application/json"
    }

async def demonstrate_routing():
    """Demonstrate automatic routing based on model and content."""
    
    print("üîÄ Image Model Routing & Token Scaling Demonstration")
    print("=" * 60)
    
    async with httpx.AsyncClient() as client:
        
        # Example 1: Text-only request ‚Üí Anthropic endpoint
        print("\n1Ô∏è‚É£  Text-only Request (glm-4.5)")
        print("   ‚Üí Routes to: Anthropic endpoint")
        print("   ‚Üí Context: 200k tokens (scaled to 128k for client)")
        
        payload = {
            "model": "glm-4.5",
            "messages": [{"role": "user", "content": "Explain quantum computing in one sentence."}],
            "max_tokens": 50,
            "stream": False
        }
        
        try:
            response = await client.post(
                f"{PROXY_BASE_URL}/v1/chat/completions",
                json=payload,
                headers=get_headers(),
                timeout=30.0
            )
            
            if response.status_code == 200:
                data = response.json()
                usage = data.get("usage", {})
                print(f"   ‚úÖ Success: {data['choices'][0]['message']['content'][:50]}...")
                print(f"   üìä Token usage: {usage.get('prompt_tokens', 0)} ‚Üí {usage.get('completion_tokens', 0)} (scaled for 128k compatibility)")
            else:
                print(f"   ‚ùå Failed: {response.status_code}")
                
        except Exception as e:
            print(f"   ‚ùå Error: {e}")
        
        # Example 2: Vision model request ‚Üí OpenAI endpoint
        print("\n2Ô∏è‚É£  Vision Model Request (glm-4.5v)")
        print("   ‚Üí Routes to: OpenAI endpoint")
        print("   ‚Üí Context: 64k tokens (scaled to 128k for client)")
        
        payload = {
            "model": "glm-4.5v", 
            "messages": [{"role": "user", "content": "Hello from the vision model!"}],
            "max_tokens": 50,
            "stream": False
        }
        
        try:
            response = await client.post(
                f"{PROXY_BASE_URL}/v1/chat/completions",
                json=payload,
                headers=get_headers(),
                timeout=30.0
            )
            
            if response.status_code == 200:
                data = response.json()
                usage = data.get("usage", {})
                print(f"   ‚úÖ Success: {data['choices'][0]['message']['content'][:50]}...")
                print(f"   üìä Token usage: {usage.get('prompt_tokens', 0)} ‚Üí {usage.get('completion_tokens', 0)} (scaled from 64k to 128k)")
            else:
                print(f"   ‚ö†Ô∏è  Routing successful, upstream response: {response.status_code}")
                if response.status_code == 500:
                    error = response.json()
                    if "insufficient balance" in str(error).lower():
                        print("   üí° This confirms routing is working (billing issue upstream)")
                        
        except Exception as e:
            print(f"   ‚ùå Error: {e}")
        
        # Example 3: Image request ‚Üí OpenAI endpoint
        print("\n3Ô∏è‚É£  Image Request (auto-detects images)")
        print("   ‚Üí Routes to: OpenAI endpoint")
        print("   ‚Üí Context: 64k tokens (scaled to 128k for client)")
        
        payload = {
            "model": "glm-4.5",  # Note: regular model, but has images
            "messages": [{
                "role": "user",
                "content": [
                    {"type": "text", "text": "What do you see in this image?"},
                    {
                        "type": "image_url",
                        "image_url": {
                            "url": f"data:image/png;base64,{SAMPLE_IMAGE_B64}"
                        }
                    }
                ]
            }],
            "max_tokens": 50,
            "stream": False
        }
        
        try:
            response = await client.post(
                f"{PROXY_BASE_URL}/v1/chat/completions",
                json=payload,
                headers=get_headers(),
                timeout=30.0
            )
            
            if response.status_code == 200:
                data = response.json()
                usage = data.get("usage", {})
                print(f"   ‚úÖ Success: {data['choices'][0]['message']['content'][:50]}...")
                print(f"   üìä Token usage: {usage.get('prompt_tokens', 0)} ‚Üí {usage.get('completion_tokens', 0)} (vision-scaled)")
            else:
                print(f"   ‚ö†Ô∏è  Routing successful, upstream response: {response.status_code}")
                
        except Exception as e:
            print(f"   ‚ùå Error: {e}")
    
    print("\n" + "=" * 60)
    print("üéØ Summary:")
    print("‚Ä¢ Text requests route to Anthropic (200k ‚Üí 128k scaling)")
    print("‚Ä¢ Vision model requests route to OpenAI (64k ‚Üí 128k scaling)")  
    print("‚Ä¢ Image content requests route to OpenAI (auto-detection)")
    print("‚Ä¢ Token counts are automatically scaled for client compatibility")
    print("‚Ä¢ All routing happens transparently via single OpenAI-compatible interface")

if __name__ == "__main__":
    if API_KEY == "your-api-key-here":
        print("‚ö†Ô∏è  Please set API_KEY environment variable")
        print("   export API_KEY=your-actual-api-key")
        exit(1)
        
    asyncio.run(demonstrate_routing())